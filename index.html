<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Exploring time-awareness in static images through contrastive learning.">
  <meta property="og:title" content="What Time Tells Us?"/>
  <meta property="og:description" content="Understanding time cues from static images using TICL."/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>Academic Project Page</title>
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">What Time Tells Us?</h1>
            <p class="subtitle">An Explorative Study of Time-Awareness from Static Images</p>
            <div class="publication-links">
              <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" class="button is-dark">Paper</a>
              <a href="https://github.com/Rathgrith/TICLearning/" class="button is-dark">Code</a>
              <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" class="button is-dark">arXiv</a>
            </div>
          </div>
        </div>
      </div>
    </div>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <h2 class="title is-3">Video Presentation</h2>
        <source src="static/videos/demo_video.mp4"
        type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2> -->
    </div>
  </div>
</section>
<!-- End teaser video -->
  <!-- </section>
  <section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <img src="static/images/teaser.png" height="100%" width="auto">
  
        </div>
    </div>
  </section> -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Key Takeaways</h2>
      Time becomes visible through illumination changes in what we see. Inspired by this, in this paper we explore the potential to learn time-of-day awareness from static images, trying to answer: <em>what time tells us?</em> 
      <ul class="content">
        <li><strong>Learning from Static Images:</strong> We train models to understand time-of-day information from single images.</li>
        <li><strong>Time-Image Contrastive Learning (TICL):</strong> Our approach connects timestamps with visual representations.</li>
        <li><strong>Strong Pretext Performance:</strong> TICL achieves state-of-the-art results in timestamp estimation.</li>
        <li><strong>Useful for Various Downstream Tasks:</strong> TICL embeddings improve tasks like time-based retrieval and video classification.</li>
      </ul>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <h2 class="title is-3">Dataset</h2>
      <figure>
        <img src="static/images/TOC_dataset.png" alt="Dataset Overview">
        <!-- <figcaption><strong>Dataset Overview:</strong> A sample distribution of timestamps in our dataset.</figcaption> -->
      </figure>
      <p>We introduce the Time-Oriented Collection (TOC) dataset, which consists of 130,906 images with reliable timestamps verified manually. This dataset enables us to analyze how time-related visual cues can be extracted from static images.</p>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <h2 class="title is-3">Methodology</h2>
      <figure>
        <img src="static/images/method.png" alt="Methodology Overview">
        <!-- <figcaption><strong>Methodology:</strong> The architecture of TICL for learning time-related visual features.</figcaption> -->
      </figure>
      <p>Our proposed method, Time-Image Contrastive Learning (TICL), employs a cross-modal contrastive learning framework. Intuitively, time correlates to many of the metaphysical concepts that can be described in natural languages, this have motivated us to align CLIP image embeddings with our clock timestamp representations, allowing our model to learn time-related patterns from rich visual semantical features. The indirect correlations inherited from CLIP have help our method to outperform previous methods taking raw geolocation/date metadata <em>(directly time-related!)</em> as additional inputs.</p>

    </div>
  </section>

  <section class="section">
    <div class="container">
      <h2 class="title is-3">Visual Results</h2>
      <div class="columns is-multiline">
        <div class="column is-half">
          <figure>
            <img src="static/images/time_esti.png" alt="Time Estimation">
            <figcaption><strong> Pretext Time Estimation task:</strong> Predicting time-of-day from static images.</figcaption>
          </figure>
        </div>
        <div class="column is-half">
          <figure>
            <img src="static/images/vsc.png" alt="Video Scene Classification">
            <figcaption><strong>Video Scene Classification:</strong> Understanding time-related video context.</figcaption>
          </figure>
        </div>
        <div class="column is-half">
          <figure>
            <img src="static/images/retrieval.png" alt="Time-based Image Retrieval">
            <figcaption><strong>Image Retrieval:</strong> Finding images based on time similarity.</figcaption>
          </figure>
        </div>
        <div class="column is-half">
          <figure>
            <img src="static/images/image_editing.png" alt="Time-aware Image Editing">
            <figcaption><strong>Image Editing:</strong> Modifying images based on inferred time cues.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
