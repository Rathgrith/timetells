<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Exploring time-awareness in static images through contrastive learning.">
  <meta property="og:title" content="What Time Tells Us?"/>
  <meta property="og:description" content="Understanding time cues from static images using TICL."/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    .narrow-container {
      max-width: 900px;
      margin: 0 auto;
    }
  </style>
  <title>Academic Project Page</title>
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><em>What Time Tells Us?</em> An Explorative Study of Time-Awareness Learned from Static Images</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://rathgrith.github.io/" target="_blank">Dongheng Lin</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=UJRtTJ0AAAAJ" target="_blank">Han Hu</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://jianbojiao.com/" target="_blank">Jianbo Jiao</a><sup>2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>UIUC, <sup>2</sup>University of Birmingham</span>
                    <!-- <span class="eql-cntrb"><small><br>First Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Rathgrith/TICLearning/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <h2 class="title is-3">Video Presentation</h2>
        <source src="static/videos/demo_video.mp4"
        type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2> -->
    </div>
  </div>
</section>
<!-- End teaser video -->
  <!-- </section>
  <section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <img src="static/images/teaser.png" height="100%" width="auto">
  
        </div>
    </div>
  </section> -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Key Takeaways</h2>
      Time becomes visible through illumination changes in what we see. Inspired by this, in this paper we explore the potential to learn time-of-day awareness from static images, trying to answer: <em>what time tells us?</em> 
      <ul class="content">
        <li><strong>Learning from Static Images:</strong> We train models to understand time-of-day information from single images.</li>
        <li><strong>Time-Image Contrastive Learning (TICL):</strong> Our approach connects timestamps with visual representations.</li>
        <li><strong>Strong Pretext Performance:</strong> TICL achieves state-of-the-art results in timestamp estimation.</li>
        <li><strong>Useful for Various Downstream Tasks:</strong> TICL embeddings improve tasks like time-based retrieval and video classification.</li>
      </ul>
    </div>
  </section>

  <section class="section">
    <div class="narrow-container">
      <h2 class="title is-3">üìïDataset</h2>
      <figure>
        <img src="static/images/TOC_dataset.png" alt="Dataset Overview">
        <!-- <figcaption><strong>Dataset Overview:</strong> A sample distribution of timestamps in our dataset.</figcaption> -->
      </figure>
      <p>We introduce the Time-Oriented Collection (TOC) dataset, which consists of 130,906 images with reliable timestamps verified manually. This dataset enables us to analyze how time-related visual cues can be extracted from static images.</p>
    </div>
  </section>

  <section class="section">
    <div class="narrow-container">
      <h2 class="title is-3">‚úçMethodology</h2>
      <figure>
        <img src="static/images/method.png" alt="Methodology Overview">
        <!-- <figcaption><strong>Methodology:</strong> The architecture of TICL for learning time-related visual features.</figcaption> -->
      </figure>
      <p>Our proposed method, Time-Image Contrastive Learning (TICL), employs a cross-modal contrastive learning framework. Intuitively, time correlates to many of the metaphysical concepts that can be described in natural languages, this have motivated us to align CLIP image embeddings with our clock timestamp representations, allowing our model to learn time-related patterns from rich visual semantical features. The indirect correlations inherited from CLIP have help our method to outperform previous methods taking raw geolocation/date metadata <em>(directly time-related!)</em> as additional inputs.</p>
      <figure>
        <img src="static/images/pretext.gif" alt="Time Estimation">
      </figure>
    </div>
  </section>

  
  <section class="section">
    <div class="narrow-container">
      <h2 class="title is-3">üîçDownstream: Time-based Image Retrieval</h2>
      <figure>
        <img src="static/images/retrieval.gif" alt="Time-based Image Retrieval">
        <figcaption>As an extension to the pretext task, the results show that TICL retrieves a higher percentage of images with smaller time errors compared to other features under zero-shot nearest-neighbour retrieval.</figcaption>
      </figure>
    </div>
  </section>

  <section class="section">
    <div class="narrow-container">
      <h2 class="title is-3">üèûÔ∏èDownstream: Video Scene Classification</h2>
      <figure>
        <img src="static/images/vsc.gif" alt="Video Scene Classification">
        <figcaption> Supported by an intuition about how time correlates with scene contexts (see t-SNE and text probabilities in the figure for evidence.), our model understanding time-related visual contexts can improve video scene classification task as we proved in various datasets.</figcaption>
      </figure>
    </div>
  </section>


  <section class="section">
    <div class="narrow-container">
      <h2 class="title is-3">üåÖDownstream: Time-aware Image Editing</h2>
      <figure>
        <img src="static/images/editing.gif" alt="Time-aware Image Editing">
        <figcaption>Our model also creates an visually related representation for different clock times throughout the day, therefore we can use the representation to guide the image editing task to create more images with more reasonable illuminations. The experiment results in the figure spans baseline editing methods on GAN and Diffusion models.</figcaption>
      </figure>
    </div>
  </section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<section class="section" id="acknowledgements">
  <div class="container is-max-desktop content">
    <h3 class="title">üôè Acknowledgements</h3>
    TBD
  </div>
</section>

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
